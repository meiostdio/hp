<!DOCTYPE html>
<html lang="ja">

<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" type="text/css" href="style.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Zen+Kaku+Gothic+New:wght@300&display=swap" rel="stylesheet">



    <title>ジェスチャーでパソコンを操作
        【MediaPipe】</title>



</head>

<body>
    <div class="wrapper">
        <div class="wh">
            <div class="head">
                <a href="../index.html">
                    <div class="rabel">MEIO STDIO</div>
                </a>
                <div class="dummy"></div>
            </div>


            <div class="article">
                <h1>ジェスチャーでパソコンを操作【MediaPipe】</h1>

                <p>
                    Pythonに慣れるための練習にもなりそう。<br>

                    結構簡単にすごい機能のプログラムが作れちゃいます。
                </p>

                <h2>
                    プログラムの概要
                </h2>

                <p>
                    親指と人差しで空中をつまみ、移動して指を離す動作でなんらかの処理を実行させるというプログラムです。<br>
                    今回のプログラムではPyAutoGUIのキーボード操作で、四方に動く指に合わせて４パターンのWindowsのショートカットキーが処理されます。
                </p>

                <h2>環境</h2>

                <p>
                    AnacondaのJupyter Notebookを開発環境としましたが、その必要がどこまであるのかよくわかってません。とりあえずPythonを実行できるとよいですが、注意点として、Google
                    ColaboratoryとMediaPipeの組み合わせは少なくとも筆者は確認していません(できたらいいな)。
                    とりあえずAnacondaをインストールし、以下のコードをAnaconda Powershell Prompt に入力します。

<pre class="prettyprint">
conda create -n mediapipe
conda activate mediapipe
pip install mediapipe</pre>

                インストールができれば、どこかにJupyter Notebookがあり、そこから実際にプログラムを書くためのファイルを作ります。<br>
                ファイル名は適当で問題ありません。
                </p>

                <h2>コードの説明</h2>

                <p>
                    MediaPipeのインストールができれば、さっそくJupyter Notebookでコーディングします。あとでソースコードを貼りますが、すべて一行のセルに入力して大丈夫です。

                </p>

                <h3>ライブラリをインポート</h3>

                <p>

<pre class="prettyprint">
import pyautogui
import cv2
import mediapipe as mp
import math
import numpy as np</pre>

                ライブラリをインポートするとあらかじめ用意された便利なプログラムを呼び出すことができます。

                </p>

                <h3>角度を求めるための関数の定義</h3>

                <p>


<pre class="prettyprint">
import pyautogui
import cv2
import mediapipe as mp
import math
import numpy as np</pre>

                degreeが関数名になります。()内に6つの変数を入れると、それらがx0からy2に対応して角度(0-180°)が計算されます。ここで入力される変数は手のランドマーク、つまり手の関節に与えられた点の座標になります。参照元(https://www.higashisalary.com/entry/numpy-angle-calc)

                </p>

                <h3>リアルタイムで手を認識</h3>

                <p>

                    MediaPipeの公式サイトからコードを引っ張ってきます。(https://google.github.io/mediapipe/solutions/hands.html)

<pre class="prettyprint">
mp_drawing = mp.solutions.drawing_utils
mp_drawing_styles = mp.solutions.drawing_styles
    
mp_hands = mp.solutions.hands
    
    cap = cv2.VideoCapture(0)
    
    with mp_hands.Hands(
 
        model_complexity=0,   
        min_detection_confidence=0.5,
        min_tracking_confidence=0.5) as hands:
    
      while cap.isOpened():
    
        success, image = cap.read()
    
        if not success:
    
          print("Ignoring empty camera frame.")
    
          # If loading a video, use 'break' instead of 'continue'.
    
          continue
   
        # To improve performance, optionally mark the image as not writeable to 
        # pass by reference.
    
        image.flags.writeable = False
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        results = hands.process(image)
    
        # Draw the hand annotations on the image.
    
        image.flags.writeable = True
        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
    
        if results.multi_hand_landmarks:
    
          for hand_landmarks in results.multi_hand_landmarks:
    
            mp_drawing.draw_landmarks(
    
                image,
                hand_landmarks,
                mp_hands.HAND_CONNECTIONS,
                mp_drawing_styles.get_default_hand_landmarks_style(),
                mp_drawing_styles.get_default_hand_connections_style())
    
        # Flip the image horizontally for a selfie-view display.
    
        cv2.imshow('MediaPipe Hands', cv2.flip(image, 1))
    
        if cv2.waitKey(5) & 0xFF == 27:
    
          break
    
    cap.release()
</pre>

                このコードをコピペしてしまえば、それだけで十分それっぽいプログラムになります。<br>
                コードの大意としては、ウェブカメラから映像が入力される間、そこで取得した画像を一コマずつOpenCVで加工し、それをMediaPipeに読み込ませ、グラフを描画し出力するというものになっています。なおEscキーを押すとプログラムは停止します。

                </p>

                <h3>イベントハンドラ</h3>

                <p>
                    ここからようやくオリジナルのコードになります。
                    条件は親指と人差し指でなにかをつまむ動作です。本来の方法は特定の手の形状を機械学習的なもので識別させるのかもしれませんが、そんな高度なことはできないので、親指先ー手首ー人差し指先の座標を取得し、この3点が作る角度の開閉がイベントの最初の条件にします。
                    そこからは早い話、以下のような流れになります。


                    <img src="../image/mp2.jpg">


                    (tmpx, tmpy)は座標の変数です。角度から移動させた方向がわかり、それに応じて処理が発生します。<br>
                    ちなみにこの座標ではx,yは0-1で表現されます。


<pre class="prettyprint">
indX=hand_landmarks.landmark[8].x
indY=hand_landmarks.landmark[8].y
            
wriX=hand_landmarks.landmark[0].x
wriY=hand_landmarks.landmark[0].y
            
thuX=hand_landmarks.landmark[4].x
thuY=hand_landmarks.landmark[4].y</pre>

                繰り返し処理(for)のなかで3点の座標、ind(人差し指)、wri(手首)、thu(親指)を取得します。

<pre class="prettyprint">
finger = 0
tmp1x, tmp1y, tmp2x, tmp2y = 0, 0, 0, 0
</pre>

                繰り返し処理の外でフラグを宣言・初期化します。

<pre class="prettyprint">
degree1 = degree(wriX, wriY, thuX, thuY, indX, indY)

if finger == 0 or finger == 3 and degree1 > 25:
   print('ready')
   finger = 1

if finger == 1 and degree1 < 5:
   print('catch')
   tmp1x = (indX + thuX)/2
   tmp1y = (indY + thuY)/2
   finger = 2

 if finger == 2 and degree1 > 10:
    print('release')
    tmp2x = (indX + thuX)/2
    tmp2y = (indY + thuY)/2
    degree2 = degree(tmp1x, tmp1y, 0, tmp1y, tmp2x, tmp2y)

    if degree2 < 60:
       pyautogui.hotkey('ctrl', 'y')
       print('右')

    if degree2 > 60 and degree2 < 120:

       if tmp1y > tmp2y:

          pyautogui.hotkey('ctrl', 'c')
          print('上')

        if tmp2y > tmp1y:

           pyautogui.hotkey('ctrl', 'v')
           print('下')

        if degree2 > 120:

           pyautogui.hotkey('ctrl', 'z')
           print('左')

           finger = 3</pre>

                これはfor文内部で座標を取得したあとに書かれます。先に宣言したfingerは指の動作の行程を順序よく認識させるために用いられます。また、tmpみたいな変数はキャッチとリリース時に取得した座標を一時的に保管するためのものです。<br>
                最初に定義した関数の計算の都合上、算出される角度は0から180までになっています。そのため、右方向は1-60°、上下方向は61-120°、左方向は121-180°という割り当てになり、角度だけでは上下は判別できません。苦し紛れの方策として、上下方向は角度に加えて、キャッチ時よりy座標が大きくなっているかどうかで上下の判別をします。<br>
                最後におまけ程度のpyauto.hotkey()を使います。引数にショートカットキーとして打ち込むキーを入力すればOKです。<br>

                </p>

                <h2>総括</h2>

                <p>
                    ひとえに手の形状を判別させるといってもそう簡単ではありませんでした。空中をつまんだりする動作は近未来なUIという感じがしますが、親指と人差し指が近づいているか離れているかくらいしか確度の高い判別をしてくれなかったため、やむを得ずこのジェスチャーにしたまでです。また、誤認識も頻繁に生じるので実用には程遠いクオリティです。ただMediaPipeといった強力なライブラリに触れることができたのはよい経験になりました。
                </p>
                <dd></dd>
                <dd></dd>
                <dd></dd>
            </div>



        </div>
    </div>

    <script src="https://cdn.jsdelivr.net/gh/google/code-prettify@master/loader/run_prettify.js?skin=desert"></script>

</body>

</html>
